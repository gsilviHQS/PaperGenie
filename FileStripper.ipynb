{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions\n",
    "\n",
    "#complete_text = functions.extract_all_text(['papers/1801.00862/NISQ-Quantum.tex']) \n",
    "complete_text = functions.extract_all_text(['papers/2201.08194/BP_shadows.tex']) \n",
    "#complete_text = functions.extract_all_text(['papers/2002.12902/main.tex']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['plain text', 'document', 'title', 'author', 'thanks', 'email', 'affiliation', 'date', 'abstract', 'equation', 'figure*', 'algorithm', 'algorithmic', 'theorem', 'definition', 'corollary', 'proof', 'figure', 'equation*', 'enumerate', 'align', 'align*', 'cases', 'split', 'widetext', 'thebibliography'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# cute the text between \\begin{document} and \\begin{bibliography}\n",
    "complete_text2 = complete_text.split('\\n')#[complete_text.find(r'\\begin{document}')+len(r'\\begin{document}'):complete_text.find(r'\\end{document}')]\n",
    "#complete_text3 = complete_text[complete_text.find(r'\\begin{document}')+len(r'\\begin{document}'):complete_text.find(r'\\end{document}')].split('\\n')\n",
    "\n",
    "possible_keywords = [\"\\\\title\", \"\\\\author\", \"\\\\email\", \"\\\\thanks\",\"\\\\affiliation\",\"\\\\date\",\"\\\\input\"]\n",
    "temp_possible_keywords = possible_keywords.copy()\n",
    "#add \\t to each possible_keywords\n",
    "for keyword in possible_keywords:\n",
    "    temp_possible_keywords.append('\\t'+keyword)\n",
    "possible_keywords = tuple(temp_possible_keywords)\n",
    "\n",
    "\n",
    "def content_in_pharentesis(first_division,complete_text,l):\n",
    "    second_division = first_division.split('}')\n",
    "    if len(second_division)==1:\n",
    "        return second_division[0] + content_in_pharentesis(complete_text[l+1],complete_text,l+1)\n",
    "    else:\n",
    "        return second_division[0]\n",
    "\n",
    "def extract_begin_to_end(complete_text,l, keyword):\n",
    "    #add lines from complete_text, starting from l+1 until you find a line starting with \\end{\n",
    "    #code:\n",
    "    if complete_text[l+1].startswith(keyword):\n",
    "        return ''\n",
    "    elif complete_text[l+1].startswith('%'):\n",
    "        return extract_begin_to_end(complete_text,l+1,keyword)\n",
    "    else:\n",
    "        return complete_text[l+1]  +extract_begin_to_end(complete_text,l+1,keyword)\n",
    "\n",
    "def loop_over(segments, opening, closing):\n",
    "    if closing in segments[0]:\n",
    "        return segments[0].split(closing)[0]\n",
    "    else:\n",
    "        return segments[0] +opening+ segments[1] + loop_over(segments[2:],opening, closing)\n",
    "\n",
    "\n",
    "text_sections = {}\n",
    "text_sections['pre_section'] = []\n",
    "text_keys = {}\n",
    "text_keys['plain text'] = []\n",
    "in_document = False\n",
    "in_section = False\n",
    "\n",
    "for l,line in enumerate(complete_text2):\n",
    "        \n",
    "    \n",
    "    if line.startswith(possible_keywords):\n",
    "            first_division = line.split('{')\n",
    "            keyword = first_division[0].replace('\\\\','')\n",
    "            content = content_in_pharentesis(first_division[1],complete_text2,l)\n",
    "            if keyword not in text_keys:\n",
    "                text_keys[keyword] = [content]\n",
    "            else:\n",
    "                text_keys[keyword].append(content)\n",
    "\n",
    "    elif line.startswith(\"\\\\begin{\"):\n",
    "        in_document = True\n",
    "        first_division = line.split('{')\n",
    "        second_division = first_division[1].split('}')\n",
    "        keyword = second_division[0]\n",
    "        content = extract_begin_to_end(complete_text2,l,'\\end{'+keyword)\n",
    "        if keyword not in text_keys:\n",
    "            text_keys[keyword] = [content]\n",
    "        else:\n",
    "            text_keys[keyword].append(content)\n",
    "    #print(r\"{}\".format(line))\n",
    "    if line.startswith((\"\\\\section\",\"\\\\subsection\")):\n",
    "        in_section = True\n",
    "        sections_started = True\n",
    "        # get the name and content of the section\n",
    "        first_division = line.split('{')\n",
    "        keyword = loop_over(segments=first_division[1:], opening='{', closing= '}' )\n",
    "        #get all the lines until the next \\section or \\subsection\n",
    "        content = extract_begin_to_end(complete_text2,l,(\"\\\\section\",\"\\\\subsection\",\"\\\\begin{thebibliography}\",\"\\\\end{document}\"))\n",
    "        keyword = r\"{}\".format(keyword)\n",
    "        if keyword not in text_sections:\n",
    "            text_sections[keyword] = [content]\n",
    "        else:\n",
    "            text_sections[keyword].append(content)\n",
    "    if line.startswith(\"\\\\end{document}\"):\n",
    "        in_document = False\n",
    "        in_section = False\n",
    "\n",
    "    if in_document and not line.startswith((\"\\\\\",\"\\t\",\"%\",\" \"*2,\" \"*3,\" \"*4,\" \"*5)):\n",
    "        if line != '':\n",
    "            text_keys['plain text'].append(line)\n",
    "            if not in_section:\n",
    "                text_sections['pre_section'].append(line)\n",
    "print(text_keys.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pre_section', 'Introduction', 'Avoiding barren plateaus in variational quantum optimization', 'Variational quantum eigensolver', 'Barren plateaus and entanglement', 'Weak barren plateaus and improved algorithm', 'Weak barren plateaus and initialization of VQE', 'Definition and relation to barren plateaus ', 'Illustration of WBP-free initialization', 'Entanglement control during optimization', 'Bounding entanglement increase at a single optimization step', 'Optimization performance with learning rate', 'Summary and Discussion', 'Acknowledgments', 'Appendix', 'Classical shadows and implementation details', 'Data acquisition via classical shadows', 'Estimating subsystem purities', 'Estimating gradients', 'Example of error accumulation in an Ising model', 'Proof of Theorem~\\\\ref{thm:linear-shadows} \\\\labelSec:proof', 'Unitary \\\\texorpdfstring{$t$}--designs}\\\\labelappx:t-design', 'Entanglement and unitary \\\\texorpdfstring{$2$}--designs}\\\\labelapp:ent', 'Bounding the expected trace distance', \"Bounding the expected second R\\\\'enyi entropy\", 'Entanglement growth and learning rate'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sections.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['24 quai Ernest-Ansermet, 1211 Geneva, Switzerland}',\n",
       " 'Variational quantum algorithms are promising algorithms for achieving quantum advantage on near-term devices. The quantum hardware is used to implement a variational wave function and measure observables, whereas the classical computer is used to store and update the variational parameters. ',\n",
       " 'The optimization landscape of expressive variational ans\\\\\"atze is however dominated by large regions in parameter space, known as barren plateaus, with vanishing gradients which prevents efficient optimization. In this work we propose a general algorithm to avoid barren plateaus in the initialization and throughout the optimization. To this end we define a notion of \\\\emph{weak barren plateaus} (WBP) based on the entropies of local reduced density matrices. The presence of WBPs can be efficiently quantified using recently introduced shadow tomography of the quantum state with a classical computer. We demonstrate that avoidance of WBPs suffices to ensure sizable gradients in the initialization. In addition, we demonstrate that decreasing the gradient step size, guided by the entropies allows to avoid WBPs during the optimization process. This paves the way for efficient barren plateau free optimization on near-term devices.']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sections['pre_section'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['plain text'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sections.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here we detail the derivation of Eq.~(\\\\ref{eq:trace_learning}). We first upper bound the trace distance via\\\\begin{align}\\\\label{eq:trace-distance}\\tT(\\\\rho_A, \\\\sigma_A) \\\\leq T(\\\\ket{\\\\psi}, \\\\ket{\\\\phi}) &= \\\\sqrt{1-f(\\\\ket{\\\\psi}, \\\\ket{\\\\phi})}, \\\\end{align}where $f$ stands for the pure state fidelity $f(\\\\ket{\\\\psi(\\\\bm{\\\\theta})}, \\\\ket{\\\\psi(\\\\bm{\\\\theta} +\\\\bm{\\\\delta})})=|\\\\bra{\\\\psi(\\\\bm{\\\\theta})} \\\\ket{\\\\psi(\\\\bm{\\\\theta} + \\\\bm{\\\\delta})}|^2$. Taylor expanding the pure state fidelity around $\\\\bm{\\\\theta}$ we get\\\\begin{equation}\\\\label{eq:fidelity-fisher}\\tf(\\\\ket{\\\\psi(\\\\bm{\\\\theta})}, \\\\ket{\\\\psi(\\\\bm{\\\\theta} +\\\\bm{\\\\delta})}) = 1-\\\\frac{1}{4}\\\\bm{\\\\delta}^T \\\\mathcal{F}(\\\\bm{\\\\theta}) \\\\bm{\\\\delta} + \\\\mathcal{O}(\\\\bm{\\\\delta}^4),\\\\end{equation}where $\\\\mathcal{F}(\\\\bm{\\\\theta})$ is the quantum Fisher information matrix (QFIM) given by\\\\begin{equation}\\\\label{eq:qfim_def}\\\\mathcal{F}_{i j}(\\\\bm{\\\\theta}) = 4 \\\\Re{\\\\bra{\\\\partial_i \\\\psi} \\\\ket{\\\\partial_j \\\\psi} - \\\\bra{\\\\partial_i \\\\psi} \\\\ket{\\\\psi} \\\\bra{\\\\psi} \\\\ket{\\\\partial_j \\\\psi}}.\\\\end{equation}Assuming $\\\\bm{\\\\delta} \\\\ll 1$ we can neglect higher order terms in $\\\\bm{\\\\delta}$ and so\\\\be\\t\\tT(\\\\rho_A, \\\\sigma_A) \\\\lesssim \\\\sqrt{\\\\frac{1}{4}\\\\bm{\\\\delta}^T \\\\mathcal{F}(\\\\bm{\\\\theta}) \\\\bm{\\\\delta}} =\\\\sqrt{\\\\frac{\\\\eta^2}{4} (\\\\nabla_{\\\\bm{\\\\theta}}E)^T \\\\mathcal{F}(\\\\bm{\\\\theta}) \\\\nabla_{\\\\bm{\\\\theta}}E}, \\\\ee\\twhere in the last equality we plugged in the parameter change under GD (Eq.~\\\\eqref{eq:gd}), $\\\\bm{\\\\delta}=-\\\\eta \\\\nabla_{\\\\bm{\\\\theta}} E$.']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sections['Entanglement growth and learning rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tex2py import tex2py\n",
    "\n",
    "# with open('papers/2101.08448/NISQ_Review.tex') as f: data = f.read()\n",
    "# toc = tex2py(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for section in toc:\n",
    "#     print(section)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('PaperGenie')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ce793aca159cde98913a24cdfe3e728ef24ad8241460d78cf0550e564d457aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
